#note to self; in future versions create a shiny app which allows the user to set the correlations manually and then we input this correlation matrix into the power analysis function
#The biggest issue by far with this package is that the function multivariatePowerAnalysis takes a lot of time to run. This function is used to design optimal multivariate experiments given some constraints by the user. It uses a simulation-based method, which is what makes it inefficient. I found that I could save on some simulations by allowing the function to take on different user-defined inputs, which then sets up different optimization schemes with various numbers of simulations (explained below). However, to run this function with enough simulations to get consistent results (the constants 'simulations' and 'iterations' need to both be around 100), it still takes up too much time. Optimizing this code will make the package usable.
#I have one possible way to save computational resources, but it requires extensive validation to make sure that it works. This would only apply to the two situations where power is set to a constant (these will be the second and third examples of a power analysis below). This is the process as I imagine it:
#Step 1) for a given effect size lambda and for one variable (p=1), run simulations across values of n to find the sample size n* which yields the power level of 0.8 (read section 3.4 in manuscript), or yields the power level set by the user
#Step 2) Repeat step 1 for only a few values of p (for instance, p = 2, 3, 4, 5) so that you can fit equation 8 to see how n* scales with p (in other words, we estimate gamma, nu, and zeta)
#Step 3) Use your estimates of gamma, zeta, and nu to project the cost of different experimental designs across values of p (equation 14, green line in figure 4). This is the major time-saving step, as if this works we will not need to run simulations for high values of p. This is the step that requires validation, how reliably can a power law fit at low levels of p predict values of that curve at high values of p? Power laws are scale free, so I hope that it is reliable, but we need to confirm this
#Step 4) Repeat steps 1 - 3 for different values of lambda. This produces a response surface of possible costs for different designs (Figure 5A)
#Step 5) Evaluate the response surface for optimal points (see Figure 5AB)
#If this doesn't work, or doesn't end up saving much time, we will need to explore other options. For example, nVec (which contains the tested sample sizes used in step 1) may only need a few values to get a good estimate of n*. See what's the smallest sample size you can get away with
#Other notes: make sure that when the package is initially installed, so too are all of the necessary packages
#bugs: what to do if power threshold is really high and requires sample sizes greater than 1000? another bug: rename p to lambda and ensure all other variables are consistent with manuscript or otherwise make sense. write in tutorial that purrr and Matrix need to be installed before running this function, need to speed this up, check if all equations are correct, make sure that some functions can call other functions within them, make progress bars more consistent across loops, are there any excess variables not being used? convert nstar to a function, will foreach work on mac? rewrite all comments, be consistent with making empty vectors. in the last case, what happens when the final set of designs doesn't have the set sample size within it?
#set up workspace
rm(list=ls())
setwd("~/Colin/Keck Center/Equivelancy/Multivariate case/Multivariate pre package")
source("multivariateProcessStability.R")
source("binLimits.R")
source("multivariatePowerAnalysis.R")
source("multivariateEquivalency.R")
source("monetary_cost_of_experiment.R")
library(ggpubr)
library(paletteer)
library(viridis)
library(parallel)
library(data.table)
library(foreach)
library(doParallel)
library(Matrix)
library(ggplot2)
library(viridis)
library(dplyr)
#import data from reference process
referenceProcess = read.csv("referenceProcess.csv", header = TRUE)
#set constants
b = 8 #number of bins
alpha = .01 #significance level used in stopping rule
simulations = 10 #number values of rho tested in simulations to find optimal sampling strategy. This and the following constant should be higher, but that's inefficient
iterations = 10 #number of simulations per parameter value of rho
varNumber = ncol(referenceProcess)
powerThreshold = .8
lambdaVec = seq(.1, .25, by = .05)#lambdaVec = seq(.1, .25, by = .025)
varVec = seq(1, 49, by = 5)#varVec = seq(1, 49, by = 3)
nVec = seq(10, 350, by = 25) #nVec = seq(10, 350, by = 5)
delta = 0.02 #0.08
omega = 1
referenceProcess
#note to self; in future versions create a shiny app which allows the user to set the correlations manually and then we input this correlation matrix into the power analysis function
#The biggest issue by far with this package is that the function multivariatePowerAnalysis takes a lot of time to run. This function is used to design optimal multivariate experiments given some constraints by the user. It uses a simulation-based method, which is what makes it inefficient. I found that I could save on some simulations by allowing the function to take on different user-defined inputs, which then sets up different optimization schemes with various numbers of simulations (explained below). However, to run this function with enough simulations to get consistent results (the constants 'simulations' and 'iterations' need to both be around 100), it still takes up too much time. Optimizing this code will make the package usable.
#I have one possible way to save computational resources, but it requires extensive validation to make sure that it works. This would only apply to the two situations where power is set to a constant (these will be the second and third examples of a power analysis below). This is the process as I imagine it:
#Step 1) for a given effect size lambda and for one variable (p=1), run simulations across values of n to find the sample size n* which yields the power level of 0.8 (read section 3.4 in manuscript), or yields the power level set by the user
#Step 2) Repeat step 1 for only a few values of p (for instance, p = 2, 3, 4, 5) so that you can fit equation 8 to see how n* scales with p (in other words, we estimate gamma, nu, and zeta)
#Step 3) Use your estimates of gamma, zeta, and nu to project the cost of different experimental designs across values of p (equation 14, green line in figure 4). This is the major time-saving step, as if this works we will not need to run simulations for high values of p. This is the step that requires validation, how reliably can a power law fit at low levels of p predict values of that curve at high values of p? Power laws are scale free, so I hope that it is reliable, but we need to confirm this
#Step 4) Repeat steps 1 - 3 for different values of lambda. This produces a response surface of possible costs for different designs (Figure 5A)
#Step 5) Evaluate the response surface for optimal points (see Figure 5AB)
#If this doesn't work, or doesn't end up saving much time, we will need to explore other options. For example, nVec (which contains the tested sample sizes used in step 1) may only need a few values to get a good estimate of n*. See what's the smallest sample size you can get away with
#Other notes: make sure that when the package is initially installed, so too are all of the necessary packages
#bugs: what to do if power threshold is really high and requires sample sizes greater than 1000? another bug: rename p to lambda and ensure all other variables are consistent with manuscript or otherwise make sense. write in tutorial that purrr and Matrix need to be installed before running this function, need to speed this up, check if all equations are correct, make sure that some functions can call other functions within them, make progress bars more consistent across loops, are there any excess variables not being used? convert nstar to a function, will foreach work on mac? rewrite all comments, be consistent with making empty vectors. in the last case, what happens when the final set of designs doesn't have the set sample size within it?
#set up workspace
rm(list=ls())
setwd("~/Colin/Keck Center/Equivelancy/Multivariate case/Multivariate pre package")
source("multivariateProcessStability.R")
source("binLimits.R")
source("multivariatePowerAnalysis.R")
source("multivariateEquivalency.R")
source("monetary_cost_of_experiment.R")
library(ggpubr)
library(paletteer)
library(viridis)
library(parallel)
library(data.table)
library(foreach)
library(doParallel)
library(Matrix)
library(ggplot2)
library(viridis)
library(dplyr)
#import data from reference process
referenceProcess = read.csv("referenceProcess.csv", header = TRUE)
#set constants
b = 8 #number of bins
alpha = .01 #significance level used in stopping rule
simulations = 10 #number values of rho tested in simulations to find optimal sampling strategy. This and the following constant should be higher, but that's inefficient
iterations = 10 #number of simulations per parameter value of rho
varNumber = ncol(referenceProcess)
powerThreshold = .8
lambdaVec = seq(.1, .25, by = .05)#lambdaVec = seq(.1, .25, by = .025)
varVec = seq(1, 49, by = 5)#varVec = seq(1, 49, by = 3)
nVec = seq(10, 350, by = 25) #nVec = seq(10, 350, by = 5)
delta = 0.02 #0.08
omega = 1
#is process in control
outOfControlCount = multivariateProcessStability(referenceProcess)
print(outOfControlCount)
print(paste("# Out of Control Points:", outOfControlCount))
if(outOfControlCount > 0){
stop("The process is not stable, Do Not Continue.");
}
#get reference bin limits for each factor
referenceLimits = binLimits(referenceProcess, b)
git init
system("git init")
system("git add .")
system("git commit -m 'Initial commit'")
system("git remote add origin https://github.com/colinmichaellynch/MultivariateEquivalency.git")
system("git push -u origin main")
getwd()
Sys.which("git")
